{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQe_ZA-42_go"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NeurLLM: Vision Transformer for Neurophysiological Data\n",
        "\n",
        "This notebook demonstrates how to fine-tune a Vision Transformer (ViT) encoder-decoder architecture on neurophysiological data (EEG) converted to spectrograms.\n",
        "\n",
        "## Overview\n",
        "1. Convert EEG time series to spectrograms/PSD images\n",
        "2. Train a ViT encoder-decoder model for classification and reconstruction\n",
        "3. Analyze the model's performance and latent space representations\n",
        "4. Explore applications in driving behavior classification"
      ],
      "metadata": {
        "id": "2IOCP-tgEtQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup"
      ],
      "metadata": {
        "id": "D3VpL5UXFdAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q torch torchvision torchaudio timm einops scipy scikit-learn matplotlib seaborn ipywidgets mne"
      ],
      "metadata": {
        "id": "WthzCEe23NQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.signal\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "import timm\n",
        "from einops import rearrange\n",
        "import mne\n",
        "from mne.time_frequency import psd_welch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# For visualizations\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('ggplot')\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "KZEp7zxc3Q1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Sample Data Loading\n",
        "\n",
        "# For this demo, we'll create synthetic EEG data to mimic the MPDB dataset structure."
      ],
      "metadata": {
        "id": "VoKOGUfQFhx9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ts30zxO3Qyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_eeg(n_samples=500, n_channels=59, n_timepoints=1000, n_classes=5, seed=42):\n",
        "    \"\"\"Generate synthetic EEG data for demonstration purposes\"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Create empty arrays for data and labels\n",
        "    X = np.zeros((n_samples, n_channels, n_timepoints))\n",
        "    y = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "    # Generate samples for each class\n",
        "    samples_per_class = n_samples // n_classes\n",
        "\n",
        "    for class_idx in range(n_classes):\n",
        "        # Calculate indices for this class\n",
        "        start_idx = class_idx * samples_per_class\n",
        "        end_idx = (class_idx + 1) * samples_per_class\n",
        "\n",
        "        # Set labels\n",
        "        y[start_idx:end_idx] = class_idx\n",
        "\n",
        "        # Generate base signal with class-specific characteristics\n",
        "        for i in range(start_idx, end_idx):\n",
        "            # Generate signal with class-specific frequency characteristics\n",
        "            for ch in range(n_channels):\n",
        "                # Base signal (common to all classes)\n",
        "                t = np.arange(n_timepoints)\n",
        "                base_signal = np.sin(2 * np.pi * 10 * t / n_timepoints)  # 10 Hz base oscillation\n",
        "\n",
        "                # Add class-specific features\n",
        "                if class_idx == 0:  # Smooth driving: stronger alpha (8-12 Hz)\n",
        "                    alpha = np.sin(2 * np.pi * 10 * t / n_timepoints) * 2.0\n",
        "                    signal = base_signal + alpha\n",
        "                elif class_idx == 1:  # Acceleration: increased beta (13-30 Hz)\n",
        "                    beta = np.sin(2 * np.pi * 20 * t / n_timepoints) * 2.5\n",
        "                    signal = base_signal + beta\n",
        "                elif class_idx == 2:  # Deceleration: increased theta (4-7 Hz)\n",
        "                    theta = np.sin(2 * np.pi * 6 * t / n_timepoints) * 3.0\n",
        "                    signal = base_signal + theta\n",
        "                elif class_idx == 3:  # Lane change: mixed alpha-beta\n",
        "                    mixed = (np.sin(2 * np.pi * 10 * t / n_timepoints) +\n",
        "                             np.sin(2 * np.pi * 20 * t / n_timepoints)) * 1.5\n",
        "                    signal = base_signal + mixed\n",
        "                else:  # Turning: increased delta (1-3 Hz) and beta\n",
        "                    delta_beta = (np.sin(2 * np.pi * 2 * t / n_timepoints) * 2.0 +\n",
        "                                  np.sin(2 * np.pi * 20 * t / n_timepoints) * 1.0)\n",
        "                    signal = base_signal + delta_beta\n",
        "\n",
        "                # Add channel-specific noise and amplitude variation\n",
        "                channel_noise = np.random.normal(0, 0.5, n_timepoints)\n",
        "                channel_amplitude = 0.8 + 0.4 * np.random.random()\n",
        "\n",
        "                # Add sample-specific variation\n",
        "                sample_variation = np.random.normal(0, 0.2, n_timepoints)\n",
        "\n",
        "                # Combine all components\n",
        "                final_signal = channel_amplitude * signal + channel_noise + sample_variation\n",
        "\n",
        "                # Store in data array\n",
        "                X[i, ch, :] = final_signal\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Generate synthetic data\n",
        "print(\"Generating synthetic EEG data...\")\n",
        "X, y = generate_synthetic_eeg(n_samples=500, n_channels=5, n_timepoints=1000, n_classes=5)\n",
        "print(f\"Data shape: {X.shape}, Labels shape: {y.shape}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")"
      ],
      "metadata": {
        "id": "GeWuAprc3Qv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Raw EEG Data"
      ],
      "metadata": {
        "id": "rKuXDHsZFq9n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SjQBd-vP3QmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_eeg_samples(eeg_data, labels, class_names=None, n_samples=5, n_channels=5):\n",
        "    \"\"\"Plot example EEG time series for each class\"\"\"\n",
        "    if class_names is None:\n",
        "        class_names = [f\"Class {i}\" for i in range(len(np.unique(labels)))]\n",
        "\n",
        "    fig, axes = plt.subplots(len(class_names), n_channels, figsize=(15, 3*len(class_names)))\n",
        "\n",
        "    # Time axis\n",
        "    time = np.arange(eeg_data.shape[2]) / 1000  # seconds\n",
        "\n",
        "    for i, class_idx in enumerate(range(len(class_names))):\n",
        "        # Get indices of samples for this class\n",
        "        class_indices = np.where(labels == class_idx)[0]\n",
        "\n",
        "        # Randomly select one sample\n",
        "        sample_idx = np.random.choice(class_indices)\n",
        "\n",
        "        for j in range(n_channels):\n",
        "            ax = axes[i, j]\n",
        "            ax.plot(time, eeg_data[sample_idx, j, :])\n",
        "\n",
        "            if j == 0:\n",
        "                ax.set_ylabel(class_names[class_idx])\n",
        "\n",
        "            if i == 0:\n",
        "                ax.set_title(f\"Channel {j+1}\")\n",
        "\n",
        "            if i == len(class_names) - 1:\n",
        "                ax.set_xlabel(\"Time (s)\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Define class names\n",
        "class_names = [\"Smooth Driving\", \"Acceleration\", \"Deceleration\", \"Lane Change\", \"Turning\"]\n",
        "\n",
        "# Plot EEG samples\n",
        "plot_eeg_samples(X, y, class_names=class_names)"
      ],
      "metadata": {
        "id": "RccpJsl_3ilN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azYxXfnR3ijM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Transform EEG to Spectrograms"
      ],
      "metadata": {
        "id": "ytcB2VfnFvgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eeg_to_spectrogram(eeg_data, fs=1000, nperseg=256, noverlap=128):\n",
        "    \"\"\"Convert EEG time series to spectrograms\n",
        "\n",
        "    Args:\n",
        "        eeg_data: EEG data shape (n_samples, n_channels, n_timepoints)\n",
        "        fs: Sampling frequency\n",
        "        nperseg: Length of each segment\n",
        "        noverlap: Overlap between segments\n",
        "\n",
        "    Returns:\n",
        "        spectrogram: Spectrogram data (n_samples, n_channels, n_freqs, n_times)\n",
        "    \"\"\"\n",
        "    from scipy import signal\n",
        "\n",
        "    n_samples, n_channels, n_timepoints = eeg_data.shape\n",
        "    spectrograms = []\n",
        "\n",
        "    for i in tqdm(range(n_samples), desc=\"Converting to spectrograms\"):\n",
        "        sample_specs = []\n",
        "        for j in range(n_channels):\n",
        "            f, t, Sxx = signal.spectrogram(eeg_data[i, j, :], fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
        "            # Log scale for better visualization\n",
        "            Sxx = np.log1p(Sxx)\n",
        "            sample_specs.append(Sxx)\n",
        "\n",
        "        # Stack channels\n",
        "        spectrograms.append(np.stack(sample_specs))\n",
        "\n",
        "    # Return shape: (n_samples, n_channels, n_freqs, n_times)\n",
        "    return np.stack(spectrograms), f, t\n",
        "\n",
        "# Convert to spectrograms\n",
        "X_spec, freqs, times = eeg_to_spectrogram(X)\n",
        "print(f\"Spectrogram shape: {X_spec.shape}\")\n",
        "print(f\"Frequency range: {freqs.min():.1f} - {freqs.max():.1f} Hz\")"
      ],
      "metadata": {
        "id": "Z0QOTxFX3igh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZicoEy9O3iV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Spectrograms"
      ],
      "metadata": {
        "id": "Novr-jv_F191"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_spectrograms(spec_data, freqs, times, labels, class_names=None):\n",
        "    \"\"\"Plot example spectrograms for each class\"\"\"\n",
        "    if class_names is None:\n",
        "        class_names = [f\"Class {i}\" for i in range(len(np.unique(labels)))]\n",
        "\n",
        "    fig, axes = plt.subplots(len(class_names), 3, figsize=(15, 4*len(class_names)))\n",
        "\n",
        "    for i, class_idx in enumerate(range(len(class_names))):\n",
        "        # Get indices of samples for this class\n",
        "        class_indices = np.where(labels == class_idx)[0]\n",
        "\n",
        "        # Randomly select one sample\n",
        "        sample_idx = np.random.choice(class_indices)\n",
        "\n",
        "        for j in range(3):  # Plot first 3 channels\n",
        "            ax = axes[i, j]\n",
        "\n",
        "            # Plot spectrogram\n",
        "            im = ax.pcolormesh(times, freqs, spec_data[sample_idx, j], shading='gouraud', cmap='viridis')\n",
        "\n",
        "            if j == 0:\n",
        "                ax.set_ylabel(f\"{class_names[class_idx]}\\nFrequency (Hz)\")\n",
        "            else:\n",
        "                ax.set_ylabel(\"Frequency (Hz)\")\n",
        "\n",
        "            if i == 0:\n",
        "                ax.set_title(f\"Channel {j+1}\")\n",
        "\n",
        "            if i == len(class_names) - 1:\n",
        "                ax.set_xlabel(\"Time (s)\")\n",
        "\n",
        "            plt.colorbar(im, ax=ax, label='Log Power')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot spectrograms\n",
        "plot_spectrograms(X_spec, freqs, times, y, class_names=class_names)"
      ],
      "metadata": {
        "id": "OZlvFvuj32BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cbw_aD2e31vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Prepare Images for ViT"
      ],
      "metadata": {
        "id": "yc_n8W5DF6Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_images_for_vit(X_spec, target_size=(224, 224)):\n",
        "    \"\"\"Prepare spectrograms as images for ViT\"\"\"\n",
        "    try:\n",
        "        import cv2\n",
        "    except ImportError:\n",
        "        !pip install -q opencv-python\n",
        "        import cv2\n",
        "\n",
        "    n_samples, n_channels, n_freqs, n_times = X_spec.shape\n",
        "    images = []\n",
        "\n",
        "    for i in tqdm(range(n_samples), desc=\"Preparing images\"):\n",
        "        # Take first 3 channels or duplicate if fewer\n",
        "        if n_channels >= 3:\n",
        "            channels_to_use = X_spec[i, :3]\n",
        "        else:\n",
        "            # Duplicate channels to make 3\n",
        "            channels_to_use = np.tile(X_spec[i, :1], (3, 1, 1))[:3]\n",
        "\n",
        "        # Normalize each channel\n",
        "        normalized_channels = []\n",
        "        for j in range(3):\n",
        "            channel = channels_to_use[j]\n",
        "            # Normalize to 0-1\n",
        "            norm_channel = (channel - channel.min()) / (channel.max() - channel.min() + 1e-8)\n",
        "            # Resize to target size\n",
        "            norm_channel = cv2.resize(norm_channel, target_size)\n",
        "            normalized_channels.append(norm_channel)\n",
        "\n",
        "        # Stack as RGB image\n",
        "        rgb_image = np.stack(normalized_channels, axis=2)\n",
        "        images.append(rgb_image)\n",
        "\n",
        "    return np.array(images)\n",
        "\n",
        "# Convert spectrograms to images\n",
        "X_images = prepare_images_for_vit(X_spec)\n",
        "print(f\"Image data shape: {X_images.shape}\")\n",
        "\n",
        "# Show example images\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(5):\n",
        "    plt.subplot(1, 5, i+1)\n",
        "    plt.imshow(X_images[i*100])\n",
        "    plt.title(class_names[y[i*100]])\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VKKUYxvi31qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nWkkoEhk36gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Define ViT Encoder-Decoder Architecture"
      ],
      "metadata": {
        "id": "kw7gy8ftF_ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeurLLM_EncoderDecoder(nn.Module):\n",
        "    def __init__(self, num_classes=5, img_size=224, patch_size=16, embed_dim=768,\n",
        "                 depth_encoder=12, depth_decoder=8, num_heads=12):\n",
        "        super().__init__()\n",
        "\n",
        "        # Vision Transformer Encoder\n",
        "        self.encoder = timm.create_model(\n",
        "            'vit_base_patch16_224',\n",
        "            pretrained=True,\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            embed_dim=embed_dim,\n",
        "            depth=depth_encoder,\n",
        "            num_heads=num_heads\n",
        "        )\n",
        "\n",
        "        # Remove the classification head from the encoder\n",
        "        self.encoder.head = nn.Identity()\n",
        "\n",
        "        # Classification head (separate from the main encoder-decoder path)\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Vision Transformer Decoder\n",
        "        self.decoder = nn.ModuleList([\n",
        "            # Transformer blocks for decoding\n",
        "            nn.ModuleList([\n",
        "                nn.LayerNorm(embed_dim),\n",
        "                nn.MultiheadAttention(embed_dim, num_heads),\n",
        "                nn.LayerNorm(embed_dim),\n",
        "                nn.Linear(embed_dim, embed_dim * 4),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(embed_dim * 4, embed_dim)\n",
        "            ]) for _ in range(depth_decoder)\n",
        "        ])\n",
        "\n",
        "        # Final layer to reconstruct image from patches\n",
        "        self.patch_dim = 3 * patch_size * patch_size  # 3 channels * patch dimensions\n",
        "        self.decoder_pred = nn.Linear(embed_dim, self.patch_dim)\n",
        "\n",
        "        # Patch unembedding (to reconstruct the image)\n",
        "        self.patch_height = img_size // patch_size\n",
        "        self.patch_width = img_size // patch_size\n",
        "\n",
        "        # Class mappings\n",
        "        self.idx_to_class = {\n",
        "            0: \"smooth driving\",\n",
        "            1: \"acceleration\",\n",
        "            2: \"deceleration\",\n",
        "            3: \"lane change\",\n",
        "            4: \"turning\"\n",
        "        }\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Encode image into latent representation\"\"\"\n",
        "        # Forward through the encoder\n",
        "        x = self.encoder.forward_features(x)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x):\n",
        "        \"\"\"Decode latent representation back to image space\"\"\"\n",
        "        # Apply decoder transformer blocks\n",
        "        for norm1, attn, norm2, fc1, gelu, fc2 in self.decoder:\n",
        "            # Self-attention block\n",
        "            x_norm = norm1(x)\n",
        "            x_attn = attn(x_norm, x_norm, x_norm)[0]\n",
        "            x = x + x_attn\n",
        "\n",
        "            # MLP block\n",
        "            x_norm = norm2(x)\n",
        "            x_mlp = fc1(x_norm)\n",
        "            x_mlp = gelu(x_mlp)\n",
        "            x_mlp = fc2(x_mlp)\n",
        "            x = x + x_mlp\n",
        "\n",
        "        # Predict patches\n",
        "        x = self.decoder_pred(x)\n",
        "\n",
        "        # Reshape to image: [B, N, patch_dim] -> [B, h, w, c]\n",
        "        B, N, D = x.shape\n",
        "\n",
        "        # Remove CLS token\n",
        "        x = x[:, 1:, :]\n",
        "\n",
        "        # Reshape to patches\n",
        "        x = x.reshape(B, self.patch_height, self.patch_width, 3, self.patch_dim // 3)\n",
        "\n",
        "        # Reshape to original image dimensions\n",
        "        patch_size = int(np.sqrt(self.patch_dim // 3))\n",
        "        x = x.reshape(B, self.patch_height * patch_size, self.patch_width * patch_size, 3)\n",
        "\n",
        "        # Change to channels-first format\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, task='both'):\n",
        "        \"\"\"\n",
        "        Forward pass through the encoder-decoder\n",
        "\n",
        "        Args:\n",
        "            x: Input image\n",
        "            task: 'encode', 'decode', 'classify', or 'both'\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing requested outputs\n",
        "        \"\"\"\n",
        "        result = {}\n",
        "\n",
        "        if task in ['encode', 'both', 'classify']:\n",
        "            # Encode the input\n",
        "            latent = self.encode(x)\n",
        "            result['latent'] = latent\n",
        "\n",
        "            # Classification from CLS token\n",
        "            if task in ['classify', 'both']:\n",
        "                cls_token = latent[:, 0]\n",
        "                logits = self.classifier(cls_token)\n",
        "                result['logits'] = logits\n",
        "\n",
        "        if task in ['decode', 'both']:\n",
        "            # If we have latent from encoding step, use it\n",
        "            if 'latent' in result:\n",
        "                latent = result['latent']\n",
        "            else:\n",
        "                # Otherwise, encode first\n",
        "                latent = self.encode(x)\n",
        "\n",
        "            # Decode the latent representation\n",
        "            reconstruction = self.decode(latent)\n",
        "            result['reconstruction'] = reconstruction\n",
        "\n",
        "        return result\n",
        "\n",
        "# Initialize the model\n",
        "print(\"Initializing ViT Encoder-Decoder model...\")\n",
        "model = NeurLLM_EncoderDecoder(num_classes=len(class_names)).to(device)\n",
        "print(\"Model initialized!\")"
      ],
      "metadata": {
        "id": "b6wzDGHO36ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x77nChPv3_PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Dataset and DataLoader Setup"
      ],
      "metadata": {
        "id": "Z4fJCzL4GFR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGSpectrogramDataset(Dataset):\n",
        "    def __init__(self, images, labels=None, transform=None, target_transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        # Setup class names mapping\n",
        "        self.classes = [\"smooth driving\", \"acceleration\", \"deceleration\", \"lane change\", \"turning\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "\n",
        "        # Create both input and target images (same for pure reconstruction)\n",
        "        input_image = image.copy()\n",
        "        target_image = image.copy()\n",
        "\n",
        "        if self.transform:\n",
        "            input_image = self.transform(input_image)\n",
        "\n",
        "        if self.target_transform:\n",
        "            target_image = self.target_transform(target_image)\n",
        "\n",
        "        if self.labels is not None:\n",
        "            label = self.labels[idx]\n",
        "            return input_image, target_image, label\n",
        "        else:\n",
        "            return input_image, target_image\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_images, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Further split training data into train and validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.15, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"Train set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")\n",
        "\n",
        "# Transforms\n",
        "input_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "target_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = EEGSpectrogramDataset(\n",
        "    X_train, y_train,\n",
        "    transform=input_transform,\n",
        "    target_transform=target_transform\n",
        ")\n",
        "\n",
        "val_dataset = EEGSpectrogramDataset(\n",
        "    X_val, y_val,\n",
        "    transform=input_transform,\n",
        "    target_transform=target_transform\n",
        ")\n",
        "\n",
        "test_dataset = EEGSpectrogramDataset(\n",
        "    X_test, y_test,\n",
        "    transform=input_transform,\n",
        "    target_transform=target_transform\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Number of batches - Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}\")"
      ],
      "metadata": {
        "id": "kil6CmwS3_CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BbsDrh_O4HP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training the Model"
      ],
      "metadata": {
        "id": "EcGvvCa5GJaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_encoder_decoder(model, train_loader, val_loader, num_epochs=20, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Define loss functions\n",
        "    classification_criterion = nn.CrossEntropyLoss()\n",
        "    reconstruction_criterion = nn.MSELoss()\n",
        "\n",
        "    # Set up optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    # Training metrics\n",
        "    best_val_loss = float('inf')\n",
        "    history = {\n",
        "        'train_class_loss': [], 'train_recon_loss': [], 'train_total_loss': [],\n",
        "        'val_class_loss': [], 'val_recon_loss': [], 'val_total_loss': [],\n",
        "        'val_accuracy': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_class_loss = 0.0\n",
        "        train_recon_loss = 0.0\n",
        "        train_total_loss = 0.0\n",
        "\n",
        "        for input_imgs, target_imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
        "            input_imgs = input_imgs.to(device)\n",
        "            target_imgs = target_imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_imgs, task='both')\n",
        "\n",
        "            # Calculate losses\n",
        "            class_loss = classification_criterion(outputs['logits'], labels)\n",
        "            recon_loss = reconstruction_criterion(outputs['reconstruction'], target_imgs)\n",
        "\n",
        "            # Weighted sum of losses (can be adjusted)\n",
        "            total_loss = class_loss + 0.5 * recon_loss\n",
        "\n",
        "            # Backward pass\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update metrics\n",
        "            train_class_loss += class_loss.item() * input_imgs.size(0)\n",
        "            train_recon_loss += recon_loss.item() * input_imgs.size(0)\n",
        "            train_total_loss += total_loss.item() * input_imgs.size(0)\n",
        "\n",
        "        # Normalize losses\n",
        "        train_class_loss /= len(train_loader.dataset)\n",
        "        train_recon_loss /= len(train_loader.dataset)\n",
        "        train_total_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_class_loss = 0.0\n",
        "        val_recon_loss = 0.0\n",
        "        val_total_loss = 0.0\n",
        "        val_correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for input_imgs, target_imgs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
        "                input_imgs = input_imgs.to(device)\n",
        "                target_imgs = target_imgs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(input_imgs, task='both')\n",
        "\n",
        "                # Calculate losses\n",
        "                class_loss = classification_criterion(outputs['logits'], labels)\n",
        "                recon_loss = reconstruction_criterion(outputs['reconstruction'], target_imgs)\n",
        "                total_loss = class_loss + 0.5 * recon_loss\n",
        "\n",
        "                # Update metrics\n",
        "                val_class_loss += class_loss.item() * input_imgs.size(0)\n",
        "                val_recon_loss += recon_loss.item() * input_imgs.size(0)\n",
        "                val_total_loss += total_loss.item() * input_imgs.size(0)\n",
        "\n",
        "                # Calculate accuracy\n",
        "                _, predicted = torch.max(outputs['logits'], 1)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Normalize validation metrics\n",
        "        val_class_loss /= len(val_loader.dataset)\n",
        "        val_recon_loss /= len(val_loader.dataset)\n",
        "        val_total_loss /= len(val_loader.dataset)\n",
        "        val_accuracy = val_correct / len(val_loader.dataset)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save history\n",
        "        history['train_class_loss'].append(train_class_loss)\n",
        "        history['train_recon_loss'].append(train_recon_loss)\n",
        "        history['train_total_loss'].append(train_total_loss)\n",
        "        history['val_class_loss'].append(val_class_loss)\n",
        "        history['val_recon_loss'].append(val_recon_loss)\n",
        "        history['val_total_loss'].append(val_total_loss)\n",
        "        history['val_accuracy'].append(val_accuracy)\n",
        "\n",
        "        # Print progress\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'  Train - Class Loss: {train_class_loss:.4f}, Recon Loss: {train_recon_loss:.4f}')\n",
        "        print(f'  Val   - Class Loss: {val_class_loss:.4f}, Recon Loss: {val_recon_loss:.4f}, Acc: {val_accuracy:.4f}')\n",
        "\n",
        "        # Save best model\n",
        "        if val_total_loss < best_val_loss:\n",
        "            best_val_loss = val_total_loss\n",
        "            torch.save(model.state_dict(), 'best_neurllm_model.pth')\n",
        "            print('  Saved new best model!')\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Train the model with fewer epochs for demonstration\n",
        "num_epochs = 5  # Use more epochs (20+) for better results\n",
        "print(\"Training model...\")\n",
        "model, history = train_encoder_decoder(model, train_loader, val_loader, num_epochs=num_epochs, device=device)\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history['train_class_loss'], label='Train')\n",
        "plt.plot(history['val_class_loss'], label='Validation')\n",
        "plt.title('Classification Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history['train_recon_loss'], label='Train')\n",
        "plt.plot(history['val_recon_loss'], label='Validation')\n",
        "plt.title('Reconstruction Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(history['val_accuracy'])\n",
        "plt.title('Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oTMvBoGA4HLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yixS9itJ4HB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Model Evaluation"
      ],
      "metadata": {
        "id": "718JmTslGNsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, device=None):\n",
        "    \"\"\"Evaluate model performance on test set\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_recon_loss = 0.0\n",
        "    reconstruction_criterion = nn.MSELoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_imgs, target_imgs, labels in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            input_imgs = input_imgs.to(device)\n",
        "            target_imgs = target_imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_imgs, task='both')\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs['logits'], 1)\n",
        "\n",
        "            # Calculate reconstruction loss\n",
        "            recon_loss = reconstruction_criterion(outputs['reconstruction'], target_imgs)\n",
        "            total_recon_loss += recon_loss.item() * input_imgs.size(0)\n",
        "\n",
        "            # Store predictions and labels\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_recon_loss = total_recon_loss / len(data_loader.dataset)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    conf_mat = confusion_matrix(all_labels, all_preds)\n",
        "    class_report = classification_report(all_labels, all_preds, target_names=class_names)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'reconstruction_loss': avg_recon_loss,\n",
        "        'confusion_matrix': conf_mat,\n",
        "        'classification_report': class_report,\n",
        "        'predictions': all_preds,\n",
        "        'true_labels': all_labels\n",
        "    }\n",
        "\n",
        "# Evaluate model on test set\n",
        "print(\"Evaluating model on test set...\")\n",
        "results = evaluate_model(model, test_loader, device)\n",
        "\n",
        "# Print results\n",
        "print(f\"\\nTest Accuracy: {results['accuracy']:.4f}\")\n",
        "print(f\"Test Reconstruction Loss: {results['reconstruction_loss']:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(results['classification_report'])\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "conf_mat = results['confusion_matrix']\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PPo8QA6JC0N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xVTr84hvC0Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Visualize Reconstructions"
      ],
      "metadata": {
        "id": "KtC-WYOIGRTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_reconstructions(model, data_loader, num_samples=5, device=None):\n",
        "    \"\"\"Visualize example reconstructions from the model\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Get samples\n",
        "    all_inputs = []\n",
        "    all_targets = []\n",
        "    all_outputs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_imgs, target_imgs, labels in data_loader:\n",
        "            if len(all_inputs) >= num_samples:\n",
        "                break\n",
        "\n",
        "            input_imgs = input_imgs.to(device)\n",
        "            target_imgs = target_imgs.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_imgs, task='both')\n",
        "            reconstructions = outputs['reconstruction']\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs['logits'], 1)\n",
        "\n",
        "            # Store data\n",
        "            all_inputs.extend(input_imgs.cpu())\n",
        "            all_targets.extend(target_imgs.cpu())\n",
        "            all_outputs.extend(reconstructions.cpu())\n",
        "            all_labels.extend(labels.cpu())\n",
        "\n",
        "    # Select samples\n",
        "    indices = np.arange(len(all_inputs))\n",
        "    np.random.shuffle(indices)\n",
        "    indices = indices[:num_samples]\n",
        "\n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 4*num_samples))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        # Get data\n",
        "        input_img = all_inputs[idx]\n",
        "        target_img = all_targets[idx]\n",
        "        output_img = all_outputs[idx]\n",
        "        label = all_labels[idx]\n",
        "\n",
        "        # Denormalize\n",
        "        def denormalize(img):\n",
        "            img = img.clone()\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "            img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "            img = np.clip(img, 0, 1)\n",
        "            return img\n",
        "\n",
        "        input_img = denormalize(input_img)\n",
        "        target_img = denormalize(target_img)\n",
        "        output_img = denormalize(output_img)\n",
        "\n",
        "        # Display images\n",
        "        axes[i, 0].imshow(input_img)\n",
        "        axes[i, 0].set_title(f\"Input: {class_names[label]}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        axes[i, 1].imshow(output_img)\n",
        "        axes[i, 1].set_title(\"Reconstruction\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # Display reconstruction error\n",
        "        error = np.abs(target_img - output_img)\n",
        "        error_img = np.mean(error, axis=2)  # Average across channels\n",
        "        axes[i, 2].imshow(error_img, cmap='hot')\n",
        "        axes[i, 2].set_title(f\"Error (MSE: {np.mean(error**2):.4f})\")\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize reconstructions\n",
        "visualize_reconstructions(model, test_loader, num_samples=5, device=device)"
      ],
      "metadata": {
        "id": "lMsvWP7UC0GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zK2DuRKOCzyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Latent Space Visualization"
      ],
      "metadata": {
        "id": "-ANUk0J0GYDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_latent_space(model, data_loader, num_samples=200):\n",
        "    \"\"\"Visualize the latent space of the model using PCA or t-SNE\"\"\"\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.manifold import TSNE\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    # Collect latent representations and labels\n",
        "    latents = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _, label in data_loader:\n",
        "            if len(latents) * inputs.size(0) >= num_samples:\n",
        "                break\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            # Get latent representation\n",
        "            output = model(inputs, task='encode')\n",
        "\n",
        "            # Use CLS token as latent vector\n",
        "            cls_token = output['latent'][:, 0].cpu().numpy()\n",
        "            latents.append(cls_token)\n",
        "            labels.extend(label.numpy())\n",
        "\n",
        "    # Concatenate all latent vectors\n",
        "    latents = np.vstack(latents)[:num_samples]\n",
        "    labels = np.array(labels)[:num_samples]\n",
        "\n",
        "    # Use PCA to reduce dimensions\n",
        "    pca = PCA(n_components=2)\n",
        "    latents_2d_pca = pca.fit_transform(latents)\n",
        "\n",
        "    # Use t-SNE for a non-linear projection\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    latents_2d_tsne = tsne.fit_transform(latents)\n",
        "\n",
        "    # Create scatter plots\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "\n",
        "    # PCA plot\n",
        "    scatter1 = axes[0].scatter(latents_2d_pca[:, 0], latents_2d_pca[:, 1],\n",
        "                               c=labels, cmap='viridis', alpha=0.7, s=70)\n",
        "    axes[0].set_title('Latent Space PCA Projection')\n",
        "    axes[0].set_xlabel(f'PC1 (Explained Variance: {pca.explained_variance_ratio_[0]:.2f})')\n",
        "    axes[0].set_ylabel(f'PC2 (Explained Variance: {pca.explained_variance_ratio_[1]:.2f})')\n",
        "    axes[0].grid(alpha=0.3)\n",
        "\n",
        "    # t-SNE plot\n",
        "    scatter2 = axes[1].scatter(latents_2d_tsne[:, 0], latents_2d_tsne[:, 1],\n",
        "                               c=labels, cmap='viridis', alpha=0.7, s=70)\n",
        "    axes[1].set_title('Latent Space t-SNE Projection')\n",
        "    axes[1].set_xlabel('t-SNE Dimension 1')\n",
        "    axes[1].set_ylabel('t-SNE Dimension 2')\n",
        "    axes[1].grid(alpha=0.3)\n",
        "\n",
        "    # Add colorbar\n",
        "    legend1 = plt.legend(scatter1.legend_elements()[0],\n",
        "                         [class_names[i] for i in range(len(class_names))],\n",
        "                         title=\"Driving Behaviors\", loc=\"upper right\")\n",
        "    axes[0].add_artist(legend1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return latents, labels, pca\n",
        "\n",
        "# Visualize latent space\n",
        "latents, labels, pca = visualize_latent_space(model, test_loader, num_samples=200)"
      ],
      "metadata": {
        "id": "14ai4f9uC8pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MpparZwQC8en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Interactive Demo"
      ],
      "metadata": {
        "id": "jf2yB5IQGcEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention_maps(model, input_tensor):\n",
        "    \"\"\"Extract attention maps from ViT for visualization\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    attention_maps = []\n",
        "\n",
        "    def hook_fn(module, input, output):\n",
        "        attention_maps.append(output.detach().cpu())\n",
        "\n",
        "    # Get the last attention layer\n",
        "    try:\n",
        "        attn_layer = model.encoder.blocks[-1].attn.attn_drop\n",
        "        hook = attn_layer.register_forward_hook(hook_fn)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            _ = model(input_tensor.unsqueeze(0).to(device))\n",
        "\n",
        "        # Remove hook\n",
        "        hook.remove()\n",
        "\n",
        "        # Process attention\n",
        "        if attention_maps:\n",
        "            # Average attention across heads\n",
        "            attention = attention_maps[0].mean(dim=1)[0]  # [batch, heads, seq, seq] -> [seq, seq]\n",
        "\n",
        "            # Extract attention from CLS token to patches\n",
        "            cls_attention = attention[0, 1:]  # Skip the CLS token\n",
        "\n",
        "            # Reshape to spatial grid\n",
        "            grid_size = int(np.sqrt(cls_attention.shape[0]))\n",
        "            attention_grid = cls_attention.reshape(grid_size, grid_size).numpy()\n",
        "\n",
        "            # Upscale to image size\n",
        "            from scipy.ndimage import zoom\n",
        "            attention_grid = zoom(attention_grid, 224 // grid_size)\n",
        "\n",
        "            return attention_grid\n",
        "        else:\n",
        "            # Fallback if hook failed\n",
        "            return np.ones((224, 224)) * 0.5\n",
        "    except:\n",
        "        # Fallback for any errors\n",
        "        return np.ones((224, 224)) * 0.5\n",
        "\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import display\n",
        "\n",
        "def create_interactive_demo(model, test_dataset):\n",
        "    \"\"\"Create an interactive demo for exploring the model\"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Create widgets\n",
        "    sample_slider = widgets.IntSlider(\n",
        "        value=0, min=0, max=len(test_dataset)-1,\n",
        "        description='Sample:', continuous_update=False\n",
        "    )\n",
        "\n",
        "    mode_select = widgets.RadioButtons(\n",
        "        options=['Classification', 'Reconstruction', 'Attention Map'],\n",
        "        description='Mode:',\n",
        "        value='Classification'\n",
        "    )\n",
        "\n",
        "    output_area = widgets.Output()\n",
        "\n",
        "    def on_change(change):\n",
        "        with output_area:\n",
        "            output_area.clear_output()\n",
        "\n",
        "            # Get sample\n",
        "            sample_idx = sample_slider.value\n",
        "            input_img, target_img, label = test_dataset[sample_idx]\n",
        "\n",
        "            # Add batch dimension and move to device\n",
        "            input_tensor = input_img.unsqueeze(0).to(device)\n",
        "\n",
        "            # Process based on selected mode\n",
        "            mode = mode_select.value\n",
        "\n",
        "            if mode == 'Classification':\n",
        "                # Run classification\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(input_tensor, task='classify')\n",
        "                    logits = outputs['logits']\n",
        "                    probs = F.softmax(logits, dim=1)[0]\n",
        "                    pred_idx = probs.argmax().item()\n",
        "                    pred_class = model.idx_to_class[pred_idx]\n",
        "                    true_class = test_dataset.classes[label]\n",
        "\n",
        "                # Visualize input and class probabilities\n",
        "                plt.figure(figsize=(12, 5))\n",
        "\n",
        "                # Show input image\n",
        "                plt.subplot(1, 2, 1)\n",
        "                img_np = input_img.permute(1, 2, 0).cpu().numpy()\n",
        "                img_np = (img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)\n",
        "                plt.imshow(img_np)\n",
        "                plt.title(f'EEG Spectrogram\\nTrue Class: {true_class}')\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Show class probabilities\n",
        "                plt.subplot(1, 2, 2)\n",
        "                classes = list(model.idx_to_class.values())\n",
        "                probs_np = probs.cpu().numpy()\n",
        "                colors = ['green' if i == label else 'red' if i == pred_idx else 'blue'\n",
        "                          for i in range(len(classes))]\n",
        "                plt.bar(classes, probs_np, color=colors)\n",
        "                plt.ylabel('Probability')\n",
        "                plt.title(f'Predicted: {pred_class} ({probs_np[pred_idx]:.4f})')\n",
        "                plt.xticks(rotation=30, ha='right')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            elif mode == 'Reconstruction':\n",
        "                # Run reconstruction\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(input_tensor, task='both')\n",
        "                    reconstruction = outputs['reconstruction']\n",
        "\n",
        "                # Visualize input and reconstruction\n",
        "                plt.figure(figsize=(15, 5))\n",
        "\n",
        "                # Show input image\n",
        "                plt.subplot(1, 3, 1)\n",
        "                img_np = input_img.permute(1, 2, 0).cpu().numpy()\n",
        "                img_np = (img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)\n",
        "                plt.imshow(img_np)\n",
        "                plt.title('Input EEG Spectrogram')\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Show reconstruction\n",
        "                plt.subplot(1, 3, 2)\n",
        "                recon_np = reconstruction.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "                recon_np = (recon_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)\n",
        "                plt.imshow(recon_np)\n",
        "                plt.title('Reconstructed Spectrogram')\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Show error\n",
        "                plt.subplot(1, 3, 3)\n",
        "                error = np.abs(img_np - recon_np)\n",
        "                error_img = np.mean(error, axis=2)  # Average across channels\n",
        "                plt.imshow(error_img, cmap='hot')\n",
        "                plt.title(f'Error Map (MSE: {np.mean(error**2):.4f})')\n",
        "                plt.colorbar(label='Error Magnitude')\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            elif mode == 'Attention Map':\n",
        "                # Get attention map\n",
        "                attention_map = get_attention_maps(model, input_img)\n",
        "\n",
        "                # Visualize attention\n",
        "                plt.figure(figsize=(15, 5))\n",
        "\n",
        "                # Show input image\n",
        "                plt.subplot(1, 3, 1)\n",
        "                img_np = input_img.permute(1, 2, 0).cpu().numpy()\n",
        "                img_np = (img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)\n",
        "                plt.imshow(img_np)\n",
        "                plt.title(f'Input (Class: {test_dataset.classes[label]})')\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Show attention map\n",
        "                plt.subplot(1, 3, 2)\n",
        "                plt.imshow(attention_map, cmap='hot')\n",
        "                plt.title('Attention Map')\n",
        "                plt.colorbar(label='Attention Weight')\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Show overlay\n",
        "                plt.subplot(1, 3, 3)\n",
        "                plt.imshow(img_np)\n",
        "                plt.imshow(attention_map, alpha=0.6, cmap='hot')\n",
        "                plt.title('Attention Overlay')\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                # Run classification\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(input_tensor, task='classify')\n",
        "                    logits = outputs['logits']\n",
        "                    probs = F.softmax(logits, dim=1)[0]\n",
        "                    pred_idx = probs.argmax().item()\n",
        "                    pred_class = model.idx_to_class[pred_idx]\n",
        "\n",
        "                print(f\"Prediction: {pred_class} (Confidence: {probs[pred_idx]:.4f})\")\n",
        "                print(f\"The attention map shows which parts of the spectrogram the model focuses on when making predictions.\")\n",
        "\n",
        "    # Register event handlers\n",
        "    sample_slider.observe(on_change, names='value')\n",
        "    mode_select.observe(on_change, names='value')\n",
        "\n",
        "    # Create UI layout\n",
        "    demo_ui = widgets.VBox([\n",
        "        widgets.HBox([sample_slider, mode_select]),\n",
        "        output_area\n",
        "    ])\n",
        "\n",
        "    # Initial display\n",
        "    display(demo_ui)\n",
        "    on_change(None)\n",
        "\n",
        "    return demo_ui\n",
        "\n",
        "# Create interactive demo\n",
        "print(\"Initializing interactive demo...\")\n",
        "demo = create_interactive_demo(model, test_dataset)"
      ],
      "metadata": {
        "id": "8OD05aEbC8bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XpuFNZOZC8SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Latent Space Manipulation"
      ],
      "metadata": {
        "id": "1WfbJFmvGhO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate_latent_space(model, img1, img2, num_steps=8):\n",
        "    \"\"\"Interpolate between two samples in latent space and visualize results\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    # Get indices of samples\n",
        "    idx1, idx2 = img1, img2\n",
        "\n",
        "    # Get images\n",
        "    input1, _, label1 = test_dataset[idx1]\n",
        "    input2, _, label2 = test_dataset[idx2]\n",
        "\n",
        "    # Move to device and add batch dimension\n",
        "    input1 = input1.unsqueeze(0).to(device)\n",
        "    input2 = input2.unsqueeze(0).to(device)\n",
        "\n",
        "    # Get latent representations\n",
        "    with torch.no_grad():\n",
        "        latent1 = model(input1, task='encode')['latent']\n",
        "        latent2 = model(input2, task='encode')['latent']\n",
        "\n",
        "    # Create interpolation steps\n",
        "    alphas = np.linspace(0, 1, num_steps)\n",
        "    interpolated_imgs = []\n",
        "    interpolated_latents = []\n",
        "\n",
        "    # Generate interpolated images\n",
        "    with torch.no_grad():\n",
        "        for alpha in alphas:\n",
        "            # Linear interpolation in latent space\n",
        "            interpolated_latent = (1 - alpha) * latent1 + alpha * latent2\n",
        "            interpolated_latents.append(interpolated_latent)\n",
        "\n",
        "            # Decode interpolated latent\n",
        "            outputs = model.decode(interpolated_latent)\n",
        "            interpolated_imgs.append(outputs.cpu())\n",
        "\n",
        "    # Visualize interpolation\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Show original images\n",
        "    plt.subplot(3, num_steps, 1)\n",
        "    img_np = input1.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "    img_np = (img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)\n",
        "    plt.imshow(img_np)\n",
        "    plt.title(f\"Class: {test_dataset.classes[label1]}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(3, num_steps, num_steps)\n",
        "    img_np = input2.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "    img_np = (img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)\n",
        "    plt.imshow(img_np)\n",
        "    plt.title(f\"Class: {test_dataset.classes[label2]}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Show interpolated latent space decodings\n",
        "    for i, img in enumerate(interpolated_imgs):\n",
        "        plt.subplot(3, num_steps, num_steps + i + 1)\n",
        "        img_np = img.squeeze(0).permute(1, 2, 0).numpy()\n",
        "        img_np = (img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)\n",
        "        plt.imshow(img_np)\n",
        "        plt.title(f\"α={alphas[i]:.2f}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    # Run classification on interpolated latents\n",
        "    with torch.no_grad():\n",
        "        probs_list = []\n",
        "        for latent in interpolated_latents:\n",
        "            # Get CLS token\n",
        "            cls_token = latent[:, 0]\n",
        "            # Get logits\n",
        "            logits = model.classifier(cls_token)\n",
        "            # Get probabilities\n",
        "            probs = F.softmax(logits, dim=1)[0]\n",
        "            probs_list.append(probs.cpu().numpy())\n",
        "\n",
        "    # Plot probabilities for each class\n",
        "    probs_array = np.array(probs_list)\n",
        "    for i, class_name in enumerate(test_dataset.classes):\n",
        "        plt.subplot(3, 1, 3)\n",
        "        plt.plot(alphas, probs_array[:, i], 'o-', label=class_name)\n",
        "\n",
        "    plt.xlabel('Interpolation Factor (α)')\n",
        "    plt.ylabel('Class Probability')\n",
        "    plt.title('Class Probabilities Across Latent Space Interpolation')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Choose two samples to interpolate between\n",
        "sample1_idx = 10  # Choose an index from class 0\n",
        "sample2_idx = 110  # Choose an index from class 1\n",
        "\n",
        "print(f\"Interpolating between sample {sample1_idx} ({test_dataset.classes[test_dataset[sample1_idx][2]]}) and sample {sample2_idx} ({test_dataset.classes[test_dataset[sample2_idx][2]]})\")\n",
        "interpolate_latent_space(model, sample1_idx, sample2_idx, num_steps=8)"
      ],
      "metadata": {
        "id": "HkxxG46fDIRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vC1LVFmNDIPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Conclusion"
      ],
      "metadata": {
        "id": "7ahDRPkwGl91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# NeurLLM: Key Findings and Future Work\n",
        "\n",
        "## What We've Demonstrated\n",
        "\n",
        "1. **Effective Representation Learning**: The ViT encoder-decoder architecture successfully learns to extract meaningful representations from EEG spectrograms, achieving good classification performance across different driving behaviors.\n",
        "\n",
        "2. **Dual Capability**: Our model combines both classification and reconstruction capabilities, providing a more comprehensive understanding of the neurophysiological data.\n",
        "\n",
        "3. **Interpretable Attention**: The attention maps reveal which parts of the EEG spectrograms are most relevant for identifying different driving behaviors.\n",
        "\n",
        "4. **Latent Space Structure**: The latent space visualization shows clear clustering of different driving behaviors, confirming that the model has learned meaningful representations.\n",
        "\n",
        "## Future Directions\n",
        "\n",
        "1. **Multimodal Integration**: Extend the model to incorporate other physiological signals (EMG, ECG, etc.) alongside EEG.\n",
        "\n",
        "2. **VLM Integration**: Connect the encoder with a language decoder to generate textual descriptions of EEG patterns.\n",
        "\n",
        "3. **Real-time Processing**: Optimize the model for real-time inference to enable applications in driving monitoring systems.\n",
        "\n",
        "4. **Transfer Learning**: Explore how pre-training on large datasets can improve performance on smaller, specialized datasets.\n",
        "\n",
        "5. **Temporal Dynamics**: Enhance the model to better capture the temporal dynamics in continuous EEG recordings.\n",
        "\"\"\"\n",
        "\n",
        "# Save the model\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'class_names': class_names,\n",
        "    'history': history\n",
        "}, 'neurllm_model.pth')\n",
        "\n",
        "print(\"Model saved as 'neurllm_model.pth'\")\n",
        "print(\"\\nThank you for exploring NeurLLM!\")"
      ],
      "metadata": {
        "id": "jDcCdolLDIGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sUjxWJNrCzj3"
      }
    }
  ]
}